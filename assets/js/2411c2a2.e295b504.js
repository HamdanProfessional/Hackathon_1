"use strict";(globalThis.webpackChunkai_robotics_web=globalThis.webpackChunkai_robotics_web||[]).push([[433],{6566:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>p,frontMatter:()=>s,metadata:()=>r,toc:()=>l});const r=JSON.parse('{"id":"en/vla/vla-introduction","title":"Vision-Language-Action Models","description":"What are Vision-Language-Action Models?","source":"@site/docs/en/vla/introduction.md","sourceDirName":"en/vla","slug":"/en/vla/vla-introduction","permalink":"/Hackathon_1/docs/en/vla/vla-introduction","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"id":"vla-introduction","title":"Vision-Language-Action Models","sidebar_position":1},"sidebar":"englishSidebar","previous":{"title":"Gazebo Simulation Basics","permalink":"/Hackathon_1/docs/en/simulation/gazebo-basics"}}');var o=t(4848),i=t(8453);const s={id:"vla-introduction",title:"Vision-Language-Action Models",sidebar_position:1},a="VLA: The Convergence of LLMs and Robotics",c={},l=[{value:"What are Vision-Language-Action Models?",id:"what-are-vision-language-action-models",level:2},{value:"The VLA Pipeline",id:"the-vla-pipeline",level:2},{value:"Voice-to-Action with OpenAI Whisper",id:"voice-to-action-with-openai-whisper",level:2},{value:"Real-time Speech Recognition",id:"real-time-speech-recognition",level:3},{value:"Cognitive Planning with LLMs",id:"cognitive-planning-with-llms",level:2},{value:"Object Detection with Computer Vision",id:"object-detection-with-computer-vision",level:2},{value:"Executing Actions with ROS 2",id:"executing-actions-with-ros-2",level:2},{value:"Multi-Modal Interaction",id:"multi-modal-interaction",level:2},{value:"Capstone Project: Autonomous Humanoid",id:"capstone-project-autonomous-humanoid",level:2}];function d(e){const n={admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"vla-the-convergence-of-llms-and-robotics",children:"VLA: The Convergence of LLMs and Robotics"})}),"\n",(0,o.jsx)(n.h2,{id:"what-are-vision-language-action-models",children:"What are Vision-Language-Action Models?"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Vision-Language-Action (VLA)"})," models represent the cutting edge of robotic AI. They combine:"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Vision"}),": Understanding the visual world (cameras, depth sensors)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Language"}),": Natural language understanding and generation (LLMs)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Action"}),": Robotic control and manipulation"]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:'VLA models enable robots to understand commands like "Pick up the red cup and place it on the table" and translate them into precise motor actions.'}),"\n",(0,o.jsx)(n.h2,{id:"the-vla-pipeline",children:"The VLA Pipeline"}),"\n",(0,o.jsx)(n.p,{children:"A complete VLA system consists of several stages:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"User Voice Command\r\n    \u2193\r\nSpeech-to-Text (Whisper)\r\n    \u2193\r\nLanguage Model (GPT-4)\r\n    \u2193\r\nTask Planning (ROS 2 Actions)\r\n    \u2193\r\nVision Processing (Object Detection)\r\n    \u2193\r\nMotion Planning (Nav2/MoveIt)\r\n    \u2193\r\nRobot Execution\n"})}),"\n",(0,o.jsx)(n.h2,{id:"voice-to-action-with-openai-whisper",children:"Voice-to-Action with OpenAI Whisper"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Whisper"})," converts speech to text with high accuracy:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import whisper\r\n\r\n# Load model\r\nmodel = whisper.load_model("base")\r\n\r\n# Transcribe audio\r\nresult = model.transcribe("command.mp3")\r\nprint(result["text"])\r\n# Output: "Pick up the red cup and place it on the table"\n'})}),"\n",(0,o.jsx)(n.h3,{id:"real-time-speech-recognition",children:"Real-time Speech Recognition"}),"\n",(0,o.jsx)(n.p,{children:"For continuous speech recognition:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import pyaudio\r\nimport numpy as np\r\n\r\n# Audio stream\r\np = pyaudio.PyAudio()\r\nstream = p.open(\r\n    format=pyaudio.paInt16,\r\n    channels=1,\r\n    rate=16000,\r\n    input=True,\r\n    frames_per_buffer=1024\r\n)\r\n\r\n# Buffer audio\r\naudio_buffer = []\r\nwhile True:\r\n    data = stream.read(1024)\r\n    audio_buffer.append(np.frombuffer(data, dtype=np.int16))\r\n\r\n    if len(audio_buffer) >= 16:  # ~1 second at 16kHz\r\n        audio = np.concatenate(audio_buffer)\r\n        result = model.transcribe(audio)\r\n        command = result["text"]\r\n        # Process command...\r\n        audio_buffer = []\n'})}),"\n",(0,o.jsx)(n.h2,{id:"cognitive-planning-with-llms",children:"Cognitive Planning with LLMs"}),"\n",(0,o.jsx)(n.p,{children:"Use GPT-4 to decompose high-level commands into ROS 2 action sequences:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'from openai import AsyncOpenAI\r\nimport os\r\n\r\nclient = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))\r\n\r\nasync def plan_task(command: str):\r\n    response = await client.chat.completions.create(\r\n        model="gpt-4",\r\n        messages=[\r\n            {"role": "system", "content": """You are a robot task planner.\r\n            Convert natural language commands into JSON action sequences.\r\n\r\n            Available actions:\r\n            - navigate(x, y): Move to coordinates\r\n            - detect_object(class): Find object by class\r\n            - grasp(): Close gripper\r\n            - release(): Open gripper\r\n            - raise_arm(height): Lift arm to height\r\n            """},\r\n            {"role": "user", "content": command}\r\n        ]\r\n    )\r\n\r\n    return response.choices[0].message.content\r\n\r\n# Example usage\r\ncommand = "Pick up the red cup and place it on the table"\r\nplan = await plan_task(command)\r\nprint(plan)\n'})}),"\n",(0,o.jsx)(n.p,{children:"Output JSON:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-json",children:'{\r\n  "actions": [\r\n    {"type": "detect_object", "params": {"class": "cup", "color": "red"}},\r\n    {"type": "navigate", "params": {"x": "object_x", "y": "object_y"}},\r\n    {"type": "grasp", "params": {}},\r\n    {"type": "raise_arm", "params": {"height": 0.5}},\r\n    {"type": "navigate", "params": {"x": "table_x", "y": "table_y"}},\r\n    {"type": "release", "params": {}}\r\n  ]\r\n}\n'})}),"\n",(0,o.jsx)(n.h2,{id:"object-detection-with-computer-vision",children:"Object Detection with Computer Vision"}),"\n",(0,o.jsx)(n.p,{children:"Integrate YOLO for real-time object detection:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'from ultralytics import YOLO\r\nimport cv2\r\n\r\nmodel = YOLO("yolov8n.pt")\r\n\r\ndef detect_objects(frame):\r\n    results = model(frame)\r\n    detections = []\r\n\r\n    for r in results:\r\n        for box in r.boxes:\r\n            cls = int(box.cls[0])\r\n            conf = float(box.conf[0])\r\n            x1, y1, x2, y2 = box.xyxy[0].tolist()\r\n\r\n            detections.append({\r\n                "class": model.names[cls],\r\n                "confidence": conf,\r\n                "bbox": [x1, y1, x2, y2]\r\n            })\r\n\r\n    return detections\n'})}),"\n",(0,o.jsx)(n.h2,{id:"executing-actions-with-ros-2",children:"Executing Actions with ROS 2"}),"\n",(0,o.jsx)(n.p,{children:"Translate planned actions to ROS 2 action clients:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import rclpy\r\nfrom rclpy.action import ActionClient\r\nfrom nav2_msgs.action import NavigateToPose\r\nfrom geometry_msgs.msg import PoseStamped\r\n\r\nclass RobotController:\r\n    def __init__(self):\r\n        self.nav_client = ActionClient(self, NavigateToPose, \'navigate_to_pose\')\r\n\r\n    async def execute_plan(self, plan):\r\n        for action in plan["actions"]:\r\n            if action["type"] == "navigate":\r\n                await self.navigate(action["params"]["x"], action["params"]["y"])\r\n            elif action["type"] == "grasp":\r\n                await self.grasp()\r\n            # ... other actions\r\n\r\n    async def navigate(self, x, y):\r\n        goal_msg = NavigateToPose.Goal()\r\n        goal_msg.pose.header.frame_id = \'map\'\r\n        goal_msg.pose.pose.position.x = x\r\n        goal_msg.pose.pose.position.y = y\r\n\r\n        await self.nav_client.wait_for_server()\r\n        send_goal_future = self.nav_client.send_goal_async(goal_msg)\r\n        goal_handle = await send_goal_future\r\n\r\n        result = await goal_handle.get_result_async()\r\n        return result.status\n'})}),"\n",(0,o.jsx)(n.h2,{id:"multi-modal-interaction",children:"Multi-Modal Interaction"}),"\n",(0,o.jsx)(n.p,{children:"Combine speech, gestures, and vision:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'class MultiModalInterface:\r\n    def __init__(self):\r\n        self.speech_recognizer = whisper.load_model("base")\r\n        self.gesture_detector = load_gesture_model()\r\n        self.vision_model = YOLO("yolov8n.pt")\r\n\r\n    async def process_input(self, audio, video_frame):\r\n        # Process speech\r\n        speech_text = self.speech_recognizer.transcribe(audio)["text"]\r\n\r\n        # Detect pointing gesture\r\n        gesture = self.gesture_detector(video_frame)\r\n\r\n        # If user is pointing, detect target object\r\n        if gesture == "pointing":\r\n            objects = self.vision_model(video_frame)\r\n            target = self.get_pointed_object(gesture.direction, objects)\r\n            command = f"{speech_text} the {target}"\r\n        else:\r\n            command = speech_text\r\n\r\n        return command\n'})}),"\n",(0,o.jsx)(n.h2,{id:"capstone-project-autonomous-humanoid",children:"Capstone Project: Autonomous Humanoid"}),"\n",(0,o.jsx)(n.p,{children:"Build a complete VLA system:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Input"}),': Voice command "Clean the room"']}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Planning"}),": LLM generates subtasks (navigate, identify objects, pick up, dispose)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Perception"}),": Vision system identifies trash objects"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Navigation"}),": Nav2 plans collision-free paths"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Manipulation"}),": MoveIt controls arm to grasp objects"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Verification"}),": Vision confirms task completion"]}),"\n"]}),"\n",(0,o.jsx)(n.admonition,{title:"Hardware Considerations",type:"tip",children:(0,o.jsx)(n.p,{children:'VLA models require significant compute for LLM inference. Click "Personalize" to see recommended deployment strategies for your hardware (RTX4090 edge inference, Jetson optimization, or cloud-based processing).'})}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Congratulations!"})," You've completed the Physical AI & Humanoid Robotics textbook fundamentals. Continue exploring the advanced topics in each module."]})]})}function p(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>a});var r=t(6540);const o={},i=r.createContext(o);function s(e){const n=r.useContext(i);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),r.createElement(i.Provider,{value:n},e.children)}}}]);